<!-- ---
title: "Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective"
collection: publications
permalink: /publications/TBD
excerpt: "Adversarial Robustness and Accuracy are Consistent in Graph Models"
date: 2023-10-21
venue: "NeurIPS"
year: 2023
authorlist: "Kuan Li, Yiwen Chen, Yang Liu, Jin Wang, Qing He, Xiang Ao"
status: 'todo'
---
**Abstract: Graph-based structural perturbations possess two distinctive characteristics: they violate certain properties of the original graph, and they are discrete. Consequently, defenses against graph attacks can leverage these properties to directly eliminate discrete perturbations by discerning between normal and adversarial edges. However, relying on specific properties renders defenses vulnerable to adaptive (white-box) attacks from adversaries with equivalent knowledge. Perturbed edges, compared to original edges in the graph, can be viewed as out-of-distribution (OOD) samples. We consider that the differentiation of perturbed edges from original edges based on specific properties inadequately captures the complete OOD phenomenon. Hence, we propose a more comprehensive approach by modeling the entire OOD problem. To this end, we propose a self-training method and a brand new adversarial training paradigm that incorporates OOD detection to enhance the robustness of GNNs against poisoning attack and evasion attack, respectively. Furthermore, we theoretically prove why traditional adversarial training on graph structure fails to improve the robustness of GNNs.  Through extensive unit tests conducted on over 25,000 perturbed graphs, we validate the adaptive robustness of our methods. We also take the adaptive design towards our defenses into account and identify a potential trade-off between attack effectiveness and defensibility in the context of graph attacks.**

TBD
 -->
